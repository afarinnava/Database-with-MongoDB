{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Naive Bayes training error for each fold =  [0.27142857 0.27142857 0.24637681 0.24637681 0.26086957 0.26086957\n",
      " 0.31884058 0.30434783 0.14492754 0.17647059]\n",
      "Question 1: Naive Bayes validation error for each fold =  [0.44444444 0.25       0.25       0.375      0.25       0.375\n",
      " 0.42857143 0.14285714 0.14285714 0.28571429]\n",
      "\n",
      "    Question 2: Is the error of naive bayes <0.2 with confidence 0.9?\n",
      "    We need to do a one-sided t-test with confidence level 90%\n",
      "    the Hypothesis are: H0: mu <= 0.2 vs. Ha: mu > 0.2\n",
      "    using the mean and standard deviation of the errors that follow\n",
      "    and t (one-tail alpha = 0.05 , df= 9)= 1.833, n = 10\n",
      "    the confidence interval that is created includes numbers that are\n",
      "    are greater than 0.2. This implies that\n",
      "    At 90% confidence level the mean of errors for Naive Bayes algorithem is\n",
      "    greater than 0.2.\n",
      "\n",
      "mean of errors =  0.2944444444444444\n",
      "confidence interval = ( 0.2349275230918079 ,  0.3539613657970809 )\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statistics as stat\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import math \n",
    "\n",
    "# import data\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "x = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, 8].values\n",
    "\n",
    "#training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.10, random_state=80, shuffle=True)\n",
    "\n",
    "\n",
    "#Question 1- Naive Bayes on training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB = GaussianNB()\n",
    "NB.fit(x_train, y_train)\n",
    "y_pred = NB.predict(x_test)\n",
    "\n",
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# calculate accuracy and error for training and validation sets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "NB_train_acc = cross_val_score(estimator = NB, X = x_train, y = y_train, cv = 10)\n",
    "NB_train_error = 1-NB_train_acc\n",
    "\n",
    "NB_valid_acc = cross_val_score(estimator = NB, X = x_test, y = y_test, cv = 10)\n",
    "NB_valid_error = 1-NB_valid_acc\n",
    "\n",
    "print('Question 1: Naive Bayes training error for each fold = ', NB_train_error\n",
    ")\n",
    "print('Question 1: Naive Bayes validation error for each fold = ', NB_valid_error\n",
    ")\n",
    "\n",
    "\n",
    "#Question 2- one-sided t-test\n",
    "print('''\n",
    "    Question 2: Is the error of naive bayes <0.2 with confidence 0.9?\n",
    "    We need to do a one-sided t-test with confidence level 90%\n",
    "    the Hypothesis are: H0: mu <= 0.2 vs. Ha: mu > 0.2\n",
    "    using the mean and standard deviation of the errors that follow\n",
    "    and t (one-tail alpha = 0.05 , df= 9)= 1.833, n = 10\n",
    "    the confidence interval that is created includes numbers that are\n",
    "    are greater than 0.2. This implies that\n",
    "    At 90% confidence level the mean of errors for Naive Bayes algorithem is\n",
    "    greater than 0.2.\n",
    "''')\n",
    "      \n",
    "NB_x_bar = NB_valid_error.mean()\n",
    "NB_std_bar = NB_valid_error.std()\n",
    "print('mean of errors = ', NB_x_bar)\n",
    "#print('std of errors = ', NB_std_bar)\n",
    "print('confidence interval = (', NB_x_bar - 1.833 * NB_std_bar / math.sqrt(10), ', ', \n",
    "      NB_x_bar + 1.833 * NB_std_bar / math.sqrt(10), ')' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Logistic regression training error for each fold =  [0.27142857 0.17142857 0.2173913  0.28985507 0.26086957 0.24637681\n",
      " 0.27536232 0.30434783 0.13043478 0.16176471]\n",
      "Question 1: Logistic regression validation error for each fold =  [0.44444444 0.25       0.125      0.25       0.125      0.375\n",
      " 0.57142857 0.14285714 0.42857143 0.14285714]\n",
      "mean of errors =  0.28035087719298246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/afarinnava/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Question 1- Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train, y_train)\n",
    "#print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "#     .format(logreg.score(x_train, y_train)))\n",
    "#print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    " #    .format(logreg.score(x_test, y_test)))\n",
    "\n",
    "y_pred = logreg.predict(x_test)\n",
    "\n",
    "#confusion matrix\n",
    "conf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# calculate accuracy and error for training and validation sets\n",
    "\n",
    "LR_train_acc = cross_val_score(estimator = logreg, X = x_train, y = y_train, cv = 10)\n",
    "LR_train_error = 1-LR_train_acc\n",
    "\n",
    "LR_valid_acc = cross_val_score(estimator = logreg, X = x_test, y = y_test, cv = 10)\n",
    "LR_valid_error = 1-LR_valid_acc\n",
    "\n",
    "print('Question 1: Logistic regression training error for each fold = ', LR_train_error\n",
    ")\n",
    "print('Question 1: Logistic regression validation error for each fold = ', LR_valid_error\n",
    ")\n",
    "\n",
    "LR_x_bar = valid_error.mean()\n",
    "LR_std_bar = valid_error.std()\n",
    "print('mean of errors = ', LR_x_bar)\n",
    "#print('std of errors = ', LR_std_bar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: 3nn classifier training error for each fold =  [0.34285714 0.34285714 0.30434783 0.26086957 0.27536232 0.34782609\n",
      " 0.4057971  0.28985507 0.30434783 0.26470588]\n",
      "Question 1: 3nn classifier validation error for each fold =  [0.22222222 0.375      0.375      0.5        0.25       0.375\n",
      " 0.42857143 0.28571429 0.42857143 0.14285714]\n",
      "mean of errors =  0.3382936507936508\n"
     ]
    }
   ],
   "source": [
    "# Question 1- 3nn classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 3) #3-nn\n",
    "knn.fit(x_train, y_train)\n",
    "#print('Accuracy of 3-NN classifier on training set: {:.2f}'\n",
    "#     .format(knn.score(X_train, y_train)))\n",
    "#print('Accuracy of 3-NN classifier on test set: {:.2f}'\n",
    "#     .format(knn.score(X_test, y_test)))\n",
    "\n",
    "y_pred = knn.predict(x_test)\n",
    "\n",
    "#confusion matrix\n",
    "conf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# calculate accuracy and error\n",
    "KN_train_acc = cross_val_score(estimator = knn, X = x_train, y = y_train, cv = 10)\n",
    "KN_train_error = 1-KN_train_acc\n",
    "\n",
    "KN_valid_acc = cross_val_score(estimator = knn, X = x_test, y = y_test, cv = 10)\n",
    "KN_valid_error = 1-KN_valid_acc\n",
    "\n",
    "print('Question 1: 3nn classifier training error for each fold = ', KN_train_error\n",
    ")\n",
    "print('Question 1: 3nn classifier validation error for each fold = ', KN_valid_error\n",
    ")\n",
    "\n",
    "KN_x_bar = KN_valid_error.mean()\n",
    "KN_std_bar = KN_valid_error.std()\n",
    "print('mean of errors = ', KN_x_bar)\n",
    "#print('std of errors = ', KN_std_bar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Question 3: Do Naive Bayes and knn have the same error? (with confidence 0.9)\n",
      "    We need to do a paired t-test with confidence level 90%\n",
      "    the Hypothesis are: H0: dif_mu(NB_mu - 3nn_mu) = 0 vs. Ha: dif_mu # 0\n",
      "    using the mean and standard deviation of the dif_errors that follow\n",
      "    and t (two-tail alpha = 0.1 , df= 9)= 1.833, n = 10\n",
      "    1) If the confidence interval that is created includes zero, then there is no difference \n",
      "    between the two algorithms at 90% confidence level, \n",
      "    2) If the interval includes negative numbers, then Naive Bayes algorithm has lower errors,\n",
      "    3) and if the interval has only positive numbers, then 3nn algorithm has lower errors.\n",
      "\n",
      "confidence interval = ( -0.126 , 0.038 )\n",
      "NB and KNN have the same error\n"
     ]
    }
   ],
   "source": [
    "# Question 3- Do Naive Bayes and knn have the same error?\n",
    "# Paired t-test with confidence level 90%\n",
    "# find the difference between the errors of NB and KNN\n",
    "dif_error = NB_valid_error - KN_valid_error\n",
    "\n",
    "# use a paired t-test for the following hypotheses\n",
    "print('''\n",
    "    Question 3: Do Naive Bayes and knn have the same error? (with confidence 0.9)\n",
    "    We need to do a paired t-test with confidence level 90%\n",
    "    the Hypothesis are: H0: dif_mu(NB_mu - 3nn_mu) = 0 vs. Ha: dif_mu # 0\n",
    "    using the mean and standard deviation of the dif_errors that follow\n",
    "    and t (two-tail alpha = 0.1 , df= 9)= 1.833, n = 10\n",
    "    1) If the confidence interval that is created includes zero, then there is no difference \n",
    "    between the two algorithms at 90% confidence level, \n",
    "    2) If the interval includes negative numbers, then Naive Bayes algorithm has lower errors,\n",
    "    3) and if the interval has only positive numbers, then 3nn algorithm has lower errors.\n",
    "''')\n",
    "      \n",
    "dif_x_bar = dif_error.mean()\n",
    "dif_std_bar = dif_error.std()\n",
    "#print('mean of errors = ', dif_x_bar)\n",
    "#print('std of errors = ', dif_std_bar)\n",
    "low = dif_x_bar - 1.833 * dif_std_bar / math.sqrt(10)\n",
    "high = dif_x_bar + 1.833 * dif_std_bar / math.sqrt(10)\n",
    "print('confidence interval = ( {:.3f}'\n",
    "     .format( low), ', {:.3f}'\n",
    "     .format(high), ')' )\n",
    "\n",
    "if 0 >= low and 0 <= high:\n",
    "    print('NB and KNN have the same error')\n",
    "elif high < 0:\n",
    "    print('KNN has higher errors')\n",
    "else:\n",
    "    print('NB has higher errors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three classifiers have the same error!\n"
     ]
    }
   ],
   "source": [
    "# Question 4- Do the three classifiers have different errors?\n",
    "# ANOVA (Analysis of Variance) on mu of each algorithm\n",
    "# There are L = 3 columns of data and k = 10 errors\n",
    "# Hypotheses: H0: NB_mu = LR_mu = KN_mu vs. \n",
    "# Ha: at least one of the means is different than others\n",
    "# avg of errors have been calculated for each algorithm\n",
    "\n",
    "import statistics\n",
    "import numpy as np\n",
    "\n",
    "# avg and variance of each alorithm\n",
    "k = 10\n",
    "L = 3\n",
    "mj = [NB_x_bar, LR_x_bar, KN_x_bar]\n",
    "m = statistics.mean(mj)\n",
    "s2 = statistics.variance(mj)\n",
    "\n",
    "# between-group sum of squares\n",
    "ssb = k * (L-1) * s2\n",
    "\n",
    "# within-group sum of squares\n",
    "sj = [NB_std_bar, LR_std_bar, KN_std_bar]\n",
    "s2j = np.power(sj, 2)\n",
    "\n",
    "ssw = (k-1) * L * statistics.mean(s2j)\n",
    "\n",
    "# Calculating F-value    \n",
    "F = (ssb/(L-1))/(ssw/(L*(k-1)))\n",
    "\n",
    "# F from table with (L-1), L*(k-1) df = F0.05, 2, 27 = 3.35\n",
    "F_table = 3.35\n",
    "\n",
    "# print the result\n",
    "if F > F_table:\n",
    "    print(\"The three classifiers DO NOT have the same error!\")\n",
    "else:\n",
    "    print(\"The three classifiers have the same error!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence interval = ( -0.066 , 0.084 )\n",
      "NB and LR have the same error\n"
     ]
    }
   ],
   "source": [
    "# Now, we can do multiple pair comparisons between algorithms\n",
    "# First, NB and LR:\n",
    "\n",
    "dif_error2 = NB_valid_error - LR_valid_error\n",
    "\n",
    "# use a paired t-test for the following hypotheses\n",
    "      \n",
    "dif_x_bar2 = dif_error2.mean()\n",
    "dif_std_bar2 = dif_error2.std()\n",
    "#print('mean of errors = ', dif_x_bar2)\n",
    "#print('std of errors = ', dif_std_bar2)\n",
    "low2 = dif_x_bar2 - 1.833 * dif_std_bar2 / math.sqrt(10)\n",
    "high2 = dif_x_bar2 + 1.833 * dif_std_bar2 / math.sqrt(10)\n",
    "print('confidence interval = ( {:.3f}'\n",
    "     .format( low2), ', {:.3f}'\n",
    "     .format(high2), ')' )\n",
    "\n",
    "if 0 >= low2 and 0 <= high2:\n",
    "    print('NB and LR have the same error')\n",
    "elif high2 < 0:\n",
    "    print('LR has higher errors')\n",
    "else:\n",
    "    print('NB has higher errors')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence interval = ( -0.033 , 0.138 )\n",
      "KN and LR have the same error\n"
     ]
    }
   ],
   "source": [
    "# Second, KN and LR:\n",
    "\n",
    "dif_error3 = KN_valid_error - LR_valid_error\n",
    "\n",
    "# use a paired t-test for the following hypotheses\n",
    "      \n",
    "dif_x_bar3 = dif_error3.mean()\n",
    "dif_std_bar3 = dif_error3.std()\n",
    "#print('mean of errors = ', dif_x_bar3)\n",
    "#print('std of errors = ', dif_std_bar3)\n",
    "low3 = dif_x_bar3 - 1.833 * dif_std_bar3 / math.sqrt(10)\n",
    "high3 = dif_x_bar3 + 1.833 * dif_std_bar3 / math.sqrt(10)\n",
    "print('confidence interval = ( {:.3f}'\n",
    "     .format( low3), ', {:.3f}'\n",
    "     .format(high3), ')' )\n",
    "\n",
    "if 0 >= low3 and 0 <= high3:\n",
    "    print('KN and LR have the same error')\n",
    "elif high3 < 0:\n",
    "    print('LR has higher errors')\n",
    "else:\n",
    "    print('KN has higher errors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>768.00</td>\n",
       "      <td>768.00</td>\n",
       "      <td>768.00</td>\n",
       "      <td>768.00</td>\n",
       "      <td>768.00</td>\n",
       "      <td>768.00</td>\n",
       "      <td>768.00</td>\n",
       "      <td>768.00</td>\n",
       "      <td>768.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>3.85</td>\n",
       "      <td>120.89</td>\n",
       "      <td>69.11</td>\n",
       "      <td>20.54</td>\n",
       "      <td>79.80</td>\n",
       "      <td>31.99</td>\n",
       "      <td>0.47</td>\n",
       "      <td>33.24</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>3.37</td>\n",
       "      <td>31.97</td>\n",
       "      <td>19.36</td>\n",
       "      <td>15.95</td>\n",
       "      <td>115.24</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0.33</td>\n",
       "      <td>11.76</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>21.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>1.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>62.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.30</td>\n",
       "      <td>0.24</td>\n",
       "      <td>24.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>3.00</td>\n",
       "      <td>117.00</td>\n",
       "      <td>72.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>30.50</td>\n",
       "      <td>32.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>29.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>6.00</td>\n",
       "      <td>140.25</td>\n",
       "      <td>80.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>127.25</td>\n",
       "      <td>36.60</td>\n",
       "      <td>0.63</td>\n",
       "      <td>41.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>17.00</td>\n",
       "      <td>199.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>846.00</td>\n",
       "      <td>67.10</td>\n",
       "      <td>2.42</td>\n",
       "      <td>81.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin     BMI  \\\n",
       "count       768.00   768.00         768.00         768.00   768.00  768.00   \n",
       "mean          3.85   120.89          69.11          20.54    79.80   31.99   \n",
       "std           3.37    31.97          19.36          15.95   115.24    7.88   \n",
       "min           0.00     0.00           0.00           0.00     0.00    0.00   \n",
       "25%           1.00    99.00          62.00           0.00     0.00   27.30   \n",
       "50%           3.00   117.00          72.00          23.00    30.50   32.00   \n",
       "75%           6.00   140.25          80.00          32.00   127.25   36.60   \n",
       "max          17.00   199.00         122.00          99.00   846.00   67.10   \n",
       "\n",
       "       DiabetesPedigreeFunction     Age  Outcome  \n",
       "count                    768.00  768.00   768.00  \n",
       "mean                       0.47   33.24     0.35  \n",
       "std                        0.33   11.76     0.48  \n",
       "min                        0.08   21.00     0.00  \n",
       "25%                        0.24   24.00     0.00  \n",
       "50%                        0.37   29.00     0.00  \n",
       "75%                        0.63   41.00     1.00  \n",
       "max                        2.42   81.00     1.00  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(data.describe(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=0, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean Of Glucose on the whole dataset is:  120 \n",
      "\n",
      "std Of Glucose on the whole dataset is:  31.96873472629156 \n",
      "\n",
      "Error of the Gaussian Naive Bayes using the formula is:  0.2597402597402597\n",
      "Error of Library Naive Bayes is:  0.2597402597402597 \n",
      "\n",
      "Error of the Gaussian Naive Bayes using the formula is:  0.23376623376623376\n",
      "Error of Library Naive Bayes is:  0.22077922077922074 \n",
      "\n",
      "Error of the Gaussian Naive Bayes using the formula is:  0.3246753246753247\n",
      "Error of Library Naive Bayes is:  0.3246753246753247 \n",
      "\n",
      "Error of the Gaussian Naive Bayes using the formula is:  0.15584415584415584\n",
      "Error of Library Naive Bayes is:  0.1558441558441559 \n",
      "\n",
      "Error of the Gaussian Naive Bayes using the formula is:  0.22077922077922077\n",
      "Error of Library Naive Bayes is:  0.23376623376623373 \n",
      "\n",
      "Error of the Gaussian Naive Bayes using the formula is:  0.3116883116883117\n",
      "Error of Library Naive Bayes is:  0.3246753246753247 \n",
      "\n",
      "Error of the Gaussian Naive Bayes using the formula is:  0.19480519480519481\n",
      "Error of Library Naive Bayes is:  0.19480519480519476 \n",
      "\n",
      "Error of the Gaussian Naive Bayes using the formula is:  0.23376623376623376\n",
      "Error of Library Naive Bayes is:  0.23376623376623373 \n",
      "\n",
      "Error of the Gaussian Naive Bayes using the formula is:  0.3157894736842105\n",
      "Error of Library Naive Bayes is:  0.3026315789473685 \n",
      "\n",
      "Error of the Gaussian Naive Bayes using the formula is:  0.23684210526315788\n",
      "Error of Library Naive Bayes is:  0.23684210526315785 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import collections \n",
    "# Question 5- Use Bayes rule to decide on the label using the Glucose feature. \n",
    "from sklearn.model_selection import KFold\n",
    "from math import sqrt\n",
    "from math import pi\n",
    "from math import exp\n",
    "from math import log\n",
    "\n",
    "X = data.iloc[:, 1].values\n",
    "Y = data.iloc[:, 8].values\n",
    "\n",
    "meanOfGlucose = statistics.mean(X)\n",
    "stdOfGlucose = statistics.stdev(X)\n",
    "\n",
    "print(\"mean Of Glucose on the whole dataset is: \", meanOfGlucose, \"\\n\")\n",
    "print(\"std Of Glucose on the whole dataset is: \", stdOfGlucose, \"\\n\")\n",
    "\n",
    "NB = GaussianNB()\n",
    "#y_pred = NB.predict(x_test)\n",
    "\n",
    "# Building 10 folds\n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, test_index in cv.split(X):\n",
    "    \n",
    "    # Define training and test sets in each class\n",
    "    X_train, X_test, Y_train, Y_test = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "\n",
    "    # Calculate probabilty of each class 0 and 1\n",
    "    counts0 = np.sum(Y_train == 0)\n",
    "    counts1 = np.sum(Y_train == 1)\n",
    "    p0 = counts0/(counts0 + counts1)    \n",
    "    p1 = counts1/(counts0 + counts1)\n",
    "\n",
    "    # caluclate the mean and std of each class of 0 in training set\n",
    "    X0 = X_train [Y_train == 0]\n",
    "    m0 = statistics.mean(X0)\n",
    "    s0 = statistics.stdev(X0)\n",
    "    \n",
    "    # caluclate the mean and std of each class of 1 in training set\n",
    "    X1 = X_train [Y_train == 1]\n",
    "    m1 = statistics.mean(X1)\n",
    "    s1 = statistics.stdev(X1) \n",
    "\n",
    "    # using Gaussing formula for Naive Base Algorithm\n",
    "    class0 = math.log(1/(math.sqrt(2*math.pi)* s0))- 1/2 * (((X_test - m0)/s0)**2) + math.log(p0)\n",
    "    class1 = math.log(1/(math.sqrt(2*math.pi)* s1))- 1/2 * (((X_test - m1)/s1)**2) + math.log(p1)\n",
    "\n",
    "    # predictions\n",
    "    pred_Y = np.greater(class1, class0).astype(int)\n",
    "    Y_diff = pred_Y - Y_test\n",
    "    \n",
    "    # count the number of correct predictions\n",
    "    countsP0 = np.sum(Y_diff == 0)\n",
    "    countsP1 = np.sum(Y_diff != 0)\n",
    "    \n",
    "    # Calculate the error of the method using Gaussian formula\n",
    "    error = countsP1 / (countsP0 + countsP1)\n",
    "    \n",
    "    print(\"Error of the Gaussian Naive Bayes using the formula is: \", error)\n",
    "    \n",
    "    # Claculating the errors using Naive Bayes Library to compare the results\n",
    "    #print(X_train.shape)\n",
    "    X_train = X_train.reshape(-1,1)\n",
    "    #print(X_train.ndim)\n",
    "    #Y_pred = NB.fit(X_train, Y_train).predict(X_test)\n",
    "    X_test = X_test.reshape(-1, 1)\n",
    "    NB.fit(X_train, Y_train)\n",
    "    error2 = 1- NB.score(X_test, Y_test)\n",
    "    print(\"Error of Library Naive Bayes is: \", error2, \"\\n\")\n",
    "    #GaussianNB()\n",
    "    ##X_test.reshape(-1, 1)\n",
    "    ##print(NB.predict_proba(X_test))\n",
    "    #print(NB.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
